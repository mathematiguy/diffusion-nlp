\section*{\centering Reproducibility Summary}

\subsubsection*{Scope of Reproducibility}
The paper discusses a new non-autoregressive language model called Diffusion-LM, which uses continuous diffusions to iteratively denoise a sequence of Gaussian vectors into word vectors. This allows for complex, controllable generation tasks, and the authors show successful control of Diffusion-LM for six challenging fine-grained control tasks.

\subsubsection*{Methodology}
I primarily relied on the author's source code for reproducing the results in the paper. While I attempted to write my own implementation, I was unable to do so within the time constraints. However, I was able to uncover some details of the authors' implementation that were not mentioned in the paper. My experiments were run on the Mila Cluster, using a single NVIDIA A100 GPU to retrain the E2E dataset model in under 10 hours. For the larger ROC stories dataset, I used 16 NVIDIA Quadro RTX 8000 GPUs to train the model in under 20 hours.

\subsubsection*{What was easy}
Re-running the original source code was straightforward. The project requirements were well-documented and the datasets were easily accessible. In total, the model required only 150 MB of text data for training and evaluation, which is much more manageable than other commonly used language modelling datasets.

\subsubsection*{What was difficult}
The model was complex and I found the paper lacking in some crucial details about how to implement it well. The paper also assumed familiarity with a wide range of literature, and there were some model details that were left out of the paper which I only discovered when I looked at the code. Replicating the experiments took a lot of trial and error, and I was unable to replicate their training time of 5 hours using the A100 GPUs in the Mila Cluster.

\subsubsection*{Communication with original authors}
I did not have contact with the original authors, but I was able to watch their poster presentation at NeurIPS 2022. I was not able to ask direct questions about their implementation, but I did discover the github issues for their code repository contained some helpful discussion.
