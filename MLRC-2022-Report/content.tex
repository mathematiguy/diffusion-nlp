\section{Introduction}

% A few sentences placing the work in high-level context. Limit it to a few paragraphs at most; your report is on reproducing a piece of work, you don’t have to motivate that work.

The original paper discusses a new non-autoregressive language model called Diffusion-LM, which is based on continuous diffusions. Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors to produce a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. The authors demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, outperforming prior work.

\section{Scope of reproducibility}
\label{sec:claims}

% Introduce the specific setting or problem addressed in this work, and list the main claims from the original paper. Think of this as writing out the main contributions of the original paper. Each claim should be relatively concise; some papers may not clearly list their claims, and one must formulate them in terms of the presented experiments. (For those familiar, these claims are roughly the scientific hypotheses evaluated in the original work.)

% A claim should be something that can be supported or rejected by your data. An example is, ``Finetuning pretrained BERT on dataset X will have higher accuracy than an LSTM trained with GloVe embeddings.''
% This is concise, and is something that can be supported by experiments.
% An example of a claim that is too vague, which can't be supported by experiments, is ``Contextual embedding models have shown strong performance on a number of tasks. We will run experiments evaluating two types of contextual embedding models on datasets X, Y, and Z."

The central claim of the paper is that a language model trained via a continuous diffusion process learns hierarchical representations, and that constraints can be placed in this latent space that allow for richer controls than those from autoregressive language models which can only condition text on the left context.

To this aim, the following control experiments are considered in the original paper:

% Each experiment in Section~\ref{sec:results} will support (at least) one of these claims, so a reader of your report should be able to separately understand the \emph{claims} and the \emph{evidence} that supports them.

\begin{itemize}
\item \textbf{Semantic Content}: Given a field and value, generate a sentence that covers the field and value, and report the success rate by exact match of the value.
\item \textbf{Parts-of-speech}: Given a sequence of parts-of-speech tags, generate a sequence of words whose POS tags match the target. Success is measured via word-level exact match.
\item \textbf{Syntax Tree}: Given a target syntactic parse tree, generate text whose syntactic parse matches the given parse. Success is evaluated using F1 scores.
\item \textbf{Syntax Spans}: Given a target pair of span and syntactic category, generate text whose parse tree over the span matches the target syntactic category. Success is measured by the fraction of spans that match exactly.
\item \textbf{Length}: Given a target length, generate a sequence with a length within ±2 of the target.
\item \textbf{Infilling}: Given left and right contexts, generate a sentence that logically connects the two. Both automatic and human evaluation is used for evaluation.
\end{itemize}

Therefore, a faithful reproduction of the original work would involve rerunning these experiments and calculating their results.

%\jdcomment{To organizers: I asked my students to connect the main claims and the experiments that supported them. For example, in this list above they could have ``Claim 1, which is supported by Experiment 1 in Figure 1.'' The benefit was that this caused the students to think about what their experiments were showing (as opposed to blindly rerunning each experiment and not considering how it fit into the overall story), but honestly it seemed hard for the students to understand what I was asking for.}

\section{Related work}
% The paper you are reproducing already has related works. Rather than just copying, focus on the relevant papers they cited (you may ignore non-relevant related works) from the perspective of the challenge. You should also include papers that were not cited by the work you are reproducing (for example, works published after/concurrently, or that were simply missed by your paper).

\section{Methodology}

% Explain your approach - did you use the author's code, or did you aim to re-implement the approach from the description in the paper? Summarize the resources (code, documentation, GPUs) that you used.

For the reproduction of the results in the paper, I primarily relied on the author's source code. While I initially attempted to write my own implementation, I was unable to do so within the time constraints. However, I was able to uncover some details of the authors' implementation that were not mentioned in the paper.

My experiments were run using a single NVIDIA A100 GPU to retrain the E2E dataset model in under 10 hours. For the larger ROC stories dataset, I used 16 NVIDIA Quadro RTX 8000 GPUs to train the model in under 20 hours. I then used the models I trained to implement the control tests described in the paper.

\subsection{Model descriptions}

Diffusion-LM is a diffusion model that involves iterative denoising of a Gaussian noise signal. During training, the text is converted into tokens by means of a default BERT tokenizer. These tokens are then embedded into a low-dimensional (e.g. $d=16$) latent space and upsampled by a fully connected network to match a Transformer encoder architecture.

As shown in Figure \ref{fig:graphical-model}, the token embeddings are iteratively noised according to a schedule parametrized by $q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t x_{t-1}}, \beta_t \mathbb{I})$. The $\beta_t$ values are chosen according to a scheduling function such that, after $T=2000$ steps, the resulting embeddings are normally distributed. This forward noise is then reversed by a Transformer model described by the Markov chain: $p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta (x_t, t))$.

The noised embeddings are then projected down to $d$ dimensions to match the input embedding size using another fully connected network. This noise removal process is applied to each sample from $x_T$ to $x_0$. A ``rounding step'' $p_\theta (w|x_0)$ is then used to map the resulting embeddings onto the nearest neighboring token using a softmax function.

\begin{figure}[h]
\includegraphics[scale=0.3]{images/diffusion-lm-graphical-model.png}
\centering
\caption{A graphical model representing the forward and reverse diffusion processes. In addition a rounding and embedding mapping is added to shift between continuous latents and the discrete token space.}
\label{fig:graphical-model}
\end{figure}

During training, a clamping trick is employed in which the denoised embeddings are clamped onto the nearest token exactly. This improved the model's stability during training, according to the authors.

The paper considered many different modifications to the diffusion process used during training, exploring hyperparameters such as the latent embedding dimension and different noise schedules. They also employed an analytical simplification of the variational lower bound loss function derived from the widely known image diffusion context.

In my work, I focused on the version from which I believe they derived their main results. The Diffusion-LM I considered is based on a Transformer architecture with 80M parameters which is basically the base bert uncased model from HuggingFace with default parameters. The input and output projection layers employed a single hidden layer of 768 dimensions (matching the embedding dimension of the BERT base model), and a tanh activation function.

\subsection{Datasets}

% For each dataset include
% 1) relevant statistics such as the number of examples and label distributions,
% 2) details of train / dev / test splits,
% 3) an explanation of any preprocessing done, and
% 4) a link to download the data (if available).

The authors trained Diffusion-LM on two datasets, E2E and ROCStories. The E2E dataset consists of 50K restaurant reviews labelled by 8 fields including food type, price and customer rating. The ROCStories dataset consists of 98K five-sentence stories, capturing a rich set of causal and temporal commonsense relations between daily events.

\subsubsection{E2E}

E2E is a dataset for training end-to-end natural language generation systems in the restaurant domain. It contains 1,026,048 tokens in total with 2,435 unique tokens, and the full dataset totals 12MB. You can download the dataset using the Huggingface datasets API or at \href{https://github.com/tuetschek/e2e-dataset}{https://github.com/tuetschek/e2e-dataset}.

The training set for the E2E dataset contains 42,061 examples, while the validation and test sets contain 4,672 and 4,693 examples respectively. Each example contains a restaurant review paired with a collection of fields and values such as named entities, food type, restaurant type and rating.

\begin{table}
  \centering
  \begin{tabular}{ | m{3cm} | m{7cm}| } 
    \hline
    Name & The Vaults \\ 
    Type & pub \\ 
    Price & more than £ 30 \\
    Customer Rating & 5 out of 5 \\
    Near & Café Adriatic \\
    Text & The Vaults pub near Café Adriatic has a 5 star rating. \\
    \hline
  \end{tabular}
  \caption{Here is one example from the E2E dataset. All values are free text values, but the fields are fixed and not all listed for every example.}
\end{table}

\begin{figure}[h]
  \includegraphics[scale=0.6]{images/e2e-field-frequencies.png}
  \centering
  \caption{In the E2E dataset, the labels occur with the following frequencies}
  \label{fig:e2e-field-frequencies}
\end{figure}

\subsubsection{ROCStories}

The ROCStories dataset consists of 98K five-sentence stories. Being the larger dataset at 22.3 MB of text, with the total raw text containing 4,319,744 tokens in total, of which 39,167 are unique. Because of its size and since it is not restricted to a single domain as E2E is, it is a richer and more complex dataset which makes it more challenging to model.

You can download the ROCStories dataset at \href{https://cs.rochester.edu/nlp/rocstories/}{https://cs.rochester.edu/nlp/rocstories/}, and some randomly selected examples are shown in figure \ref{fig:roc-stories-examples}.

\begin{table}
  \centering
  \begin{tabular}{ | m{12cm}| } 
    \hline
    I have a wolf named Kiera, but we nicknamed her Wolfie. She wanted to get up early this morning to go for a walk. She was so excited when she saw a squirrel that she pulled really hard. She almost caught the squirrel, but missed by only a few inches. She was sad when we got home, because she didn't kill the squirrel. \\ \hline
    Katie felt like chicken for lunch. On her lunch break, she left to go to a restaurant. Katie was walking across the lot when a fish hit her on her head. Experts said a sea gull probably dropped the fish while it was flying. Katie thought it was funny until her neck started to hurt. \\ \hline
    I accidentally left the webcam running on my work computer. I discovered it running when I came to work the next morning. I played back the video from the previous evening. On the video I was shocked to find something unusual. It was footage of the cleanup crew eating the apple on my desk! \\
    \hline
  \end{tabular}
  \caption{Here are three randomly selected examples from the ROCStories dataset.}
  \label{fig:roc-stories-examples}
\end{table}

\subsection{Hyperparameters}
% Describe how the hyperparameter values were set. If there was a hyperparameter search done, be sure to include the range of hyperparameters searched over, the method used to search (e.g. manual search, random search, Bayesian optimization, etc.), and the best hyperparameters found. Include the number of total experiments (e.g. hyperparameter trials). You can also include all results from that search (not just the best-found results).

\subsubsection{Diffusion-LM hyperparameters}
Diffusion-LM is a language model that uses a Transformer denoiser with specific hyperparameters, including the number of diffusion steps, the architecture, the embedding dimension, and the noise schedule. In the paper, the authors set the number of diffusion steps to 2000, the architecture to BERT-base, and the sequence length to 64. The embedding dimension was set to 16 for the E2E dataset and 128 for ROCStories. For the noise schedule, they used a sqrt schedule because it was more robust to different model parametrizations and embedding dimensions.

\subsubsection{Training hyperparameters}
To train Diffusion-LM, the authors used the AdamW optimizer with a linearly decaying learning rate initialized at 1e-4, a dropout rate of 0.1, a batch size of 64, and a total number of training iterations of 200,000 for the E2E dataset and 800,000 for the ROCStories dataset.

\subsubsection{Controllable generation hyperparameters}

To control the generation of text with Diffusion-LM, the authors ran gradient updates on the continuous latents of the model. They used the AdaGrad optimizer with a learning rate of 0.0001 and annealed the learning rate at 200,000 steps for the E2E dataset and 400,000 steps for the ROCStories dataset. This allowed them to control the generation of text by updating the latent variables of the model.

\subsection{Experimental setup and code}

% Include a description of how the experiments were set up that's clear enough a reader could replicate the setup.
% Include a description of the specific measure used to evaluate the experiments (e.g. accuracy, precision@K, BLEU score, etc.). 
% Provide a link to your code.

The original authors code was forked at \href{https://github.com/mathematiguy/Diffusion-LM}{https://github.com/mathematiguy/Diffusion-LM} and a Singularity container was built to containerize it fully, including all third party software dependencies. This container is documented in the repository so it can be rebuilt on the users machine, as it is quite large.

The experiments were separated into training Diffusion-LM separately on the E2E and ROCStories datasets, and then separately training the classifier for downstream controlled generation tasks.

Instructions are provided so that the reader can build the container locally and then run the experiments on a single node. However in practice we did extra work to deploy the model onto a compute cluster where it can be trained in distributed mode, and these scripts are also provided in case the reader may find them useful.

\subsection{Computational requirements}
% Include a description of the hardware used, such as the GPU or CPU the experiments were run on. 
% For each model, include a measure of the average runtime (e.g. average time to predict labels for a given validation set with a particular batch size).
% For each experiment, include the total computational requirements (e.g. the total GPU hours spent).
% (Note: you'll likely have to record this as you run your experiments, so it's better to think about it ahead of time). Generally, consider the perspective of a reader who wants to use the approach described in the paper --- list what they would find useful.

While the original paper states that it was possible to train the model on E2E in under 5 hours on a single 80 GB NVidia A100 GPU, which is similar in computational power to 6-8 high end consumer GPUs running in distributed mode. However, we were able to train the model on the E2E dataset successfully, but we found number of hours required for training on our A100 was closer to 10 hours. We did reach out to the authors to ask if there were details missing from their recommended hyperparameters which could explain this discrepancy, but have not received a reply at present.

Since the ROCStories dataset is trained for 4 times longer than E2E, this made training it as well impractical in the time available, considering the likely need to train the model multiple times due to subtle errors in the experimental setup that are difficult to predict ahead of time.

Based on this, assuming a user with a single lower end consumer GPU, you could reasonably expect training Diffusion-LM on E2E to take about 3 days, which is not impossibly long. However, training the model on ROCStories with the provided hyperparameters would be expected to take closer to 2 weeks.

\section{Results}
\label{sec:results}

% Start with a high-level overview of your results. Do your results support the main claims of the original paper? Keep this section as factual and precise as possible, reserve your judgement and discussion points for the next "Discussion" section.

\subsection{Results reproducing original paper}
% For each experiment, say 1) which claim in Section~\ref{sec:claims} it supports, and 2) if it successfully reproduced the associated experiment in the original paper. 
% For example, an experiment training and evaluating a model on a dataset may support a claim that that model outperforms some baseline.
% Logically group related results into sections. 

I was not able to replicate the paper in full, but I was able to train the E2E Diffusion LM model from end to end using a single A100 GPU. So far the only claim I was able to investigate from the original paper was the claim that the model should take 5 hours to train, which I found to be closer to ten hours.


%% \subsubsection{Result 1}

%% \subsubsection{Result 2}

%% \subsection{Results beyond original paper}

 
%% \subsubsection{Additional Result 1}
%% \subsubsection{Additional Result 2}

\section{Discussion}

Give your judgement on if your experimental results support the claims of the paper. Discuss the strengths and weaknesses of your approach - perhaps you didn't have time to run all the experiments, or perhaps you did additional experiments that further strengthened the claims in the paper.

\subsection{What was easy}
% Give your judgement of what was easy to reproduce. Perhaps the author's code is clearly written and easy to run, so it was easy to verify the majority of original claims. Or, the explanation in the paper was really easy to follow and put into code. 

% Be careful not to give sweeping generalizations. Something that is easy for you might be difficult to others. Put what was easy in context and explain why it was easy (e.g. code had extensive API documentation and a lot of examples that matched experiments in papers).

The authors code was easy to run, but also drew heavily from other work on diffusion models from other contexts, such as images. Their code also included the infrastructure necessary to run the model in distributed mode without much modification.

The paper also contained a lot of useful detail, and I found the code to be consistent with their stated claims for the model design. The authors also provided the command line expressions necessary to rerun their experiments using their codebase. They were also responsive on their github issues, where I was able to find some useful discussion from other people also attempting to reproduce the model for their domain.

\subsection{What was difficult}

Since the code was written by researchers, it includes many extraneous features for exploring a wide range of hyperparameters and other fine optimisation tweaks.

List part of the reproduction study that took more time than you anticipated or you felt were difficult.

Be careful to put your discussion in context. For example, don't say "the maths was difficult to follow", say "the math requires advanced knowledge of calculus to follow". 

\subsection{Communication with original authors}
Document the extent of (or lack of) communication with the original authors. To make sure the reproducibility report is a fair assessment of the original research we recommend getting in touch with the original authors. You can ask authors specific questions, or if you don't have any questions you can send them the full report to get their feedback before it gets published. 
