{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2266187-df83-47a0-afad-1331eb8d87d3",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "In this notebook I define the model object for the Diffusion LM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0568787-98eb-4d56-9eb3-bab4b07f76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ae250f-adf8-4915-a0b5-e5f7605af5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_noise_schedule(t, T=2000, s=1e-4):\n",
    "    alpha = 1 - np.sqrt(t / T + s)\n",
    "    return np.sqrt(1 - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda92ac6-ff1d-41e1-99d0-bb75fe52e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period)\n",
    "        * torch.arange(start=0, end=half, dtype=torch.float32)\n",
    "        / half\n",
    "    )\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f627082-68da-4061-b350-b32cb299ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model=\"bert-base-uncased\",\n",
    "        T=2000,  # diffusion steps\n",
    "        d=16,  # embedding dimensions\n",
    "        lr=1e-4,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(base_model)\n",
    "        self.embedding = nn.Embedding(self.tokenizer.vocab_size, d)\n",
    "        self.bert_config = BertConfig()\n",
    "        self.encoder = BertEncoder(self.bert_config)\n",
    "        self.hidden_dim = d\n",
    "        self.diffusion_steps = T\n",
    "        self.time_embed_dim = 4 * d\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = self.bert_config.hidden_size\n",
    "        self.LayerNorm = nn.LayerNorm(\n",
    "            self.hidden_size, eps=self.bert_config.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        # Add position embeddings\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.bert_config.max_position_embeddings).expand((1, -1)),\n",
    "        )\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "                self.bert_config.max_position_embeddings, self.hidden_size\n",
    "        )\n",
    "\n",
    "        # Add time embedding\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(d, self.time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.time_embed_dim, self.hidden_size),\n",
    "        )\n",
    "\n",
    "        # Downsample input vector\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(d, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "        )\n",
    "\n",
    "        # Downsample output vector\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, d),\n",
    "        )\n",
    "        \n",
    "    def get_embedding(self, text):\n",
    "        '''\n",
    "        Returns the downsampled token embedding sequence for the given `words`\n",
    "        '''\n",
    "        # Convert text to tokens\n",
    "        tokens = model.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        seq_length = tokens.size(1)\n",
    "\n",
    "        # Get d-dimensional token embeddings\n",
    "        embeddings = self.embedding(tokens)\n",
    "        return embeddings\n",
    "    \n",
    "    def forward_diffusion(self, x, T):\n",
    "        pass\n",
    "\n",
    "    def forward(self, text, timestep):\n",
    "\n",
    "        # Convert text to tokens\n",
    "        embeddings = self.get_embedding(text)\n",
    "        seq_length = embeddings.size(1)\n",
    "\n",
    "        # Upsample to `hidden_size` dimensional embeddings\n",
    "        upsampled = self.input_projection(embeddings)\n",
    "        print(f\"upsampled.shape: {upsampled.shape}\")\n",
    "\n",
    "        # Add timestep embedding + unroll across each sequence\n",
    "        timesteps = self.time_embedding(timestep_embedding(timestep, self.hidden_dim))\n",
    "        timesteps = timesteps.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        print(f\"timestep.shape: {timesteps.shape}\")\n",
    "\n",
    "        # Calculate positional embedding\n",
    "        position_embeddings = self.position_embeddings(\n",
    "            self.position_ids[:, :seq_length]\n",
    "        )\n",
    "        print(f\"position_embeddings.shape: {position_embeddings.shape}\")\n",
    "\n",
    "        # Apply dropout + layernorm\n",
    "        encoder_inputs = self.dropout(\n",
    "            self.LayerNorm(upsampled + timesteps + position_embeddings)\n",
    "        )\n",
    "\n",
    "        # Get `hidden_size`-dimensional bert representation\n",
    "        representations = model.encoder(encoder_inputs).last_hidden_state\n",
    "        print(f\"representations.shape: {representations.shape}\")\n",
    "\n",
    "        # Downsample to d-representation\n",
    "        downsampled = self.output_projection(representations)\n",
    "\n",
    "        return downsampled\n",
    "    \n",
    "    def fit(self, train_dataset, epochs, batch_size):\n",
    "        \n",
    "        for epoch in epochs:\n",
    "            \n",
    "            for batch in train_dataset:\n",
    "                \n",
    "                for text in batch:\n",
    "                \n",
    "                    # Embed the provided text\n",
    "                    embedding = self.get_embedding(text)\n",
    "\n",
    "                    # Calculate the forward diffusion steps\n",
    "                    diffused_embeddings = self.forward_diffusion(embedding, T=self.diffusion_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2f505f9-a4e9-4900-84dd-0a1e659603cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffusionLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2901a741-e82a-4a7e-9ce6-5399b2fda4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsampled.shape: torch.Size([1, 14, 768])\n",
      "timestep.shape: torch.Size([1, 14, 768])\n",
      "position_embeddings.shape: torch.Size([1, 14, 768])\n",
      "representations.shape: torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.6522e-01,  2.3675e-01,  2.6880e-02, -1.2815e-01,  1.8887e-01,\n",
       "          -8.4596e-02,  8.7145e-02,  2.6924e-01, -3.9145e-01,  3.8011e-01,\n",
       "           6.2855e-02, -3.1256e-01,  1.9716e-01,  2.9512e-02,  1.0252e-01,\n",
       "           1.5824e-01],\n",
       "         [-4.0389e-01, -1.0364e-01, -2.9069e-01, -7.6002e-02, -1.6227e-01,\n",
       "           1.8669e-01,  6.3557e-02,  3.6291e-01, -4.8197e-01,  1.7780e-01,\n",
       "           2.1744e-01,  4.3620e-02,  1.2060e-01, -1.8396e-01, -4.1723e-02,\n",
       "          -8.8776e-02],\n",
       "         [-3.5117e-01,  5.6830e-02, -1.4911e-01, -4.2641e-01,  2.2400e-02,\n",
       "           2.4115e-01,  5.4709e-01,  4.7731e-01, -5.8525e-03,  1.0133e-01,\n",
       "          -3.9800e-01,  4.1449e-01,  4.5522e-01,  8.5924e-02,  2.0514e-02,\n",
       "           2.2399e-01],\n",
       "         [-4.1458e-01, -1.1675e-01,  1.6530e-01,  2.9977e-03,  7.2667e-02,\n",
       "           2.5901e-01, -1.5625e-01,  4.8887e-01, -5.4611e-01,  1.1773e-01,\n",
       "          -3.2767e-03,  5.0806e-02,  5.3547e-02,  4.4382e-03,  2.5977e-01,\n",
       "           2.1639e-01],\n",
       "         [-3.1667e-02,  4.5920e-01, -8.4944e-04,  3.0660e-03, -1.6561e-01,\n",
       "           1.6318e-01,  1.8138e-01,  1.3359e-01, -3.5069e-01,  4.4495e-01,\n",
       "          -1.7822e-01,  3.9028e-01,  1.6312e-01, -1.0320e-01, -4.4559e-01,\n",
       "           2.9441e-01],\n",
       "         [-3.0665e-01,  3.1402e-01, -4.3623e-02, -5.0015e-01, -1.8440e-01,\n",
       "           2.1745e-01, -5.7352e-02,  4.7278e-01, -2.2007e-01,  1.6620e-02,\n",
       "           2.2026e-01,  2.4030e-03,  3.6383e-01, -2.3602e-01, -4.8726e-02,\n",
       "          -2.1058e-01],\n",
       "         [-4.1519e-01, -2.6770e-01,  7.6367e-02, -7.0933e-01, -2.7397e-01,\n",
       "          -5.8050e-02,  1.2907e-01,  9.3323e-02, -4.0617e-01,  3.4700e-01,\n",
       "          -1.4061e-01,  2.7270e-01,  1.6094e-01, -3.3080e-02, -3.0752e-03,\n",
       "           9.7107e-02],\n",
       "         [-1.8676e-01,  5.7410e-01, -1.8611e-01, -1.5460e-01,  8.4322e-03,\n",
       "          -5.7996e-02,  4.9162e-02,  2.5065e-01, -1.4246e-01,  4.1309e-01,\n",
       "          -1.4793e-01,  1.2267e-01, -1.4424e-01, -4.5790e-01, -2.2073e-01,\n",
       "          -8.1116e-02],\n",
       "         [-3.7440e-01, -2.8332e-01,  1.7892e-01, -6.9362e-02, -1.8638e-01,\n",
       "           7.7806e-02, -2.3407e-01,  1.9018e-01, -3.1455e-01,  4.8195e-01,\n",
       "           4.9159e-02, -1.2275e-01, -4.6342e-02, -2.6153e-01,  1.7238e-01,\n",
       "           2.4660e-01],\n",
       "         [-3.7078e-01, -2.7373e-01, -3.5892e-01,  2.6367e-01, -2.3420e-01,\n",
       "           5.0330e-01, -1.4454e-01,  3.9445e-01, -4.7192e-01,  5.1708e-03,\n",
       "           2.4282e-02,  2.1139e-01,  3.1673e-01, -9.0977e-02, -2.0891e-01,\n",
       "           2.0825e-01],\n",
       "         [ 7.1686e-05,  6.0008e-02,  7.5410e-02,  2.5680e-03,  1.0560e-01,\n",
       "           3.2701e-01, -2.8736e-01,  3.2264e-01, -6.4157e-01,  1.9691e-01,\n",
       "           6.1376e-02,  8.2480e-02,  9.6387e-02, -2.4336e-01, -5.7112e-02,\n",
       "          -4.5089e-02],\n",
       "         [-5.9412e-01,  2.2071e-01,  2.3976e-01,  1.4724e-01,  3.7537e-01,\n",
       "          -3.3509e-01, -2.0126e-01,  1.3369e-01, -2.9298e-01,  9.0732e-02,\n",
       "           8.7904e-02,  3.3575e-01, -1.8986e-01, -1.9714e-01,  1.3340e-01,\n",
       "           2.9866e-01],\n",
       "         [-2.0515e-01, -4.6464e-02,  8.0026e-02, -5.0910e-02,  1.2704e-01,\n",
       "           3.3216e-01, -4.8949e-01,  1.4787e-01, -2.1309e-01, -1.2655e-01,\n",
       "           3.5978e-01, -3.6244e-01,  3.6507e-01, -6.6111e-02,  2.6895e-03,\n",
       "           1.1352e-01],\n",
       "         [-2.8813e-01,  3.4046e-01,  1.4972e-01, -1.3393e-01,  3.2123e-01,\n",
       "           1.0074e-01, -1.2577e-01,  4.7140e-01,  2.8401e-02,  1.6960e-02,\n",
       "          -3.1421e-01,  1.9479e-01,  4.1939e-01, -1.7877e-01,  2.1317e-01,\n",
       "          -2.3413e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diffusion time step\n",
    "t = torch.tensor([0])\n",
    "example_text = \"In a hole in the ground there lived a hobbit\"\n",
    "output = model.forward(example_text, timestep=t)\n",
    "print(output.shape)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3615e-5013-4d6c-92a6-7095a3b6e552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
