{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2266187-df83-47a0-afad-1331eb8d87d3",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "In this notebook I define the model object for the Diffusion LM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0568787-98eb-4d56-9eb3-bab4b07f76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from einops import repeat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusion_lm.data import E2EDataset\n",
    "from diffusion_lm.model import DiffusionLM\n",
    "\n",
    "from diffusion_lm.utils import timestep_embedding\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a0f71e6-d09f-4348-bb17-7c8cb4dbd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model=\"bert-base-uncased\",\n",
    "        T=2000,  # diffusion steps\n",
    "        d=16,  # embedding dimensions\n",
    "        lr=1e-4,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(base_model)\n",
    "        self.embedding = nn.Embedding(self.tokenizer.vocab_size, d)\n",
    "        self.bert_config = BertConfig()\n",
    "        self.encoder = BertEncoder(self.bert_config)\n",
    "        self.hidden_dim = d\n",
    "        self.diffusion_steps = T\n",
    "        self.time_embed_dim = 4 * d\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = self.bert_config.hidden_size\n",
    "        self.LayerNorm = nn.LayerNorm(\n",
    "            self.hidden_size, eps=self.bert_config.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        # Add time embedding\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(d, self.time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.time_embed_dim, self.hidden_size),\n",
    "        )\n",
    "\n",
    "        # Calculate timestep embeddings\n",
    "        self.timestep_embeddings = self.get_timestep_embeddings()\n",
    "\n",
    "        # Add position embeddings\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.bert_config.max_position_embeddings).expand((1, -1)),\n",
    "        )\n",
    "        self.position_embeddings = nn.Embedding(\n",
    "            self.bert_config.max_position_embeddings, self.hidden_size\n",
    "        )\n",
    "\n",
    "        # Downsample input vector\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(d, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "        )\n",
    "\n",
    "        # Downsample output vector\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, 2 * d),\n",
    "        )\n",
    "\n",
    "    def get_timestep_embeddings(self):\n",
    "        timesteps = torch.arange(self.diffusion_steps)\n",
    "        timesteps = timestep_embedding(timesteps, self.hidden_dim)\n",
    "        timesteps = self.time_embedding(timesteps)\n",
    "        return timesteps\n",
    "\n",
    "    def q_sample(self, x, T):\n",
    "        \"\"\"\n",
    "        Otherwise known as q\n",
    "        \"\"\"\n",
    "        n_batches, seq_length, embed_dim = x.shape\n",
    "\n",
    "        # Repeat x along time dimension T times\n",
    "        x_t = einops.repeat(x, \"b s x -> b t s x\", t=2000)\n",
    "\n",
    "        # Calculate and propagate noise schedule\n",
    "        beta_t = torch.Tensor(diffusion_noise_schedule(np.arange(T)))\n",
    "        beta_t = einops.repeat(\n",
    "            beta_t, \"t -> b t w x\", b=n_batches, w=seq_length, x=embed_dim\n",
    "        )\n",
    "\n",
    "        # Generate noised samples\n",
    "        q_t = torch.normal(\n",
    "            (1 - torch.sqrt(beta_t)) * x_t, std=1 - torch.sqrt(1 - beta_t)\n",
    "        )\n",
    "\n",
    "        return q_t\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        \"\"\"\n",
    "        Otherwise known as p\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert text to tokens\n",
    "        n_batches, n_timesteps, seq_length, embed_dim = embeddings.shape\n",
    "\n",
    "        # Upsample to `hidden_size` dimensional embeddings\n",
    "        upsampled = self.input_projection(embeddings)\n",
    "        logging.debug(f\"upsampled.shape: {upsampled.shape}\")\n",
    "\n",
    "        # Add timestep embedding + unroll across each sequence\n",
    "        timesteps = einops.repeat(\n",
    "            timesteps, \"t e -> b t s e\", b=n_batches, s=seq_length\n",
    "        )\n",
    "        logging.debug(f\"timestep.shape: {timesteps.shape}\")\n",
    "\n",
    "        # Calculate positional embedding\n",
    "        position_embeddings = self.position_embeddings(\n",
    "            self.position_ids[:, :seq_length]\n",
    "        )\n",
    "        logging.debug(f\"position_embeddings.shape: {position_embeddings.shape}\")\n",
    "\n",
    "        # Apply dropout + layernorm\n",
    "        encoder_inputs = self.dropout(\n",
    "            self.LayerNorm(upsampled + timesteps + position_embeddings)\n",
    "        )\n",
    "\n",
    "        # Get `hidden_size`-dimensional bert representation\n",
    "        encoder_inputs = einops.rearrange(encoder_inputs, \"b t s x -> (b t) s x\")\n",
    "\n",
    "        encoded = self.encoder(encoder_inputs).last_hidden_state\n",
    "        logging.debug(f\"encoded.shape: {encoded.shape}\")\n",
    "\n",
    "        # Downsample to d-representation\n",
    "        downsampled = self.output_projection(encoded)\n",
    "\n",
    "        return downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2f505f9-a4e9-4900-84dd-0a1e659603cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset e2e_nlg (/home/kakapo/.cache/huggingface/datasets/e2e_nlg/default/0.0.0/bfeceb720929c2705bd227d1cfe5eaaab102a0bdac10dad618dac1e00c737430)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77156bfb8754049af29794155f42684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DiffusionLM()\n",
    "e2e_dataset = E2EDataset(\"train\")\n",
    "e2e_dataloader = DataLoader(e2e_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "973bc2d8-46ca-430e-894e-97a96b33da06",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DiffusionLM' object has no attribute 'get_embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m example_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn a hole in the ground there lived a hobbit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embedding\u001b[49m(example_text)\n\u001b[1;32m      5\u001b[0m noised_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mq_sample(embedding, \u001b[38;5;241m2000\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1265\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DiffusionLM' object has no attribute 'get_embedding'"
     ]
    }
   ],
   "source": [
    "# diffusion time step\n",
    "\n",
    "embedding = model.get_embedding(example_text)\n",
    "noised_embeddings = model.q_sample(embedding, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bdfb3ab-3fd7-4905-94d2-391956defeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to tokens\n",
    "embeddings = noised_embeddings\n",
    "seq_length = embeddings.size(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47ecb175-4ee8-4928-a3b1-7d136294a337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2000, 14, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upsample to `hidden_size` dimensional embeddings\n",
    "upsampled = model.input_projection(embeddings)\n",
    "upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3baae730-26dc-4382-971e-3378cb3ee1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches, seq_length, embed_dim = embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59dd0bc1-320d-4150-b78a-530906878cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "timestep = torch.arange(T).unsqueeze(1).repeat(n_batches, 1).reshape(n_batches, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "32154e86-33a2-445d-aadf-abe7b351d8eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2000) must match the size of tensor b (8) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add timestep embedding + unroll across each sequence\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtime_embedding(\u001b[43mtimestep_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_dim\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m timesteps\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m timesteps\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/code/diffusion_lm/utils.py:26\u001b[0m, in \u001b[0;36mtimestep_embedding\u001b[0;34m(timesteps, dim, max_period)\u001b[0m\n\u001b[1;32m     20\u001b[0m half \u001b[38;5;241m=\u001b[39m dim \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     21\u001b[0m freqs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m-\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(max_period)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39mhalf, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m/\u001b[39m half\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 26\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mtimesteps\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfreqs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[1;32m     27\u001b[0m embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mcos(args), torch\u001b[38;5;241m.\u001b[39msin(args)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2000) must match the size of tensor b (8) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Add timestep embedding + unroll across each sequence\n",
    "timesteps = model.time_embedding(timestep_embedding(timestep, model.hidden_dim))\n",
    "timesteps = timesteps.unsqueeze(2).expand(-1, seq_length, -1)\n",
    "timesteps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17889449-019a-45bf-83fc-530067623279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate positional embedding\n",
    "position_embeddings = self.position_embeddings(\n",
    "    self.position_ids[:, :seq_length]\n",
    ")\n",
    "logging.debug(f\"position_embeddings.shape: {position_embeddings.shape}\")\n",
    "\n",
    "# Apply dropout + layernorm\n",
    "encoder_inputs = self.dropout(\n",
    "    self.LayerNorm(upsampled + timesteps + position_embeddings)\n",
    ")\n",
    "\n",
    "# Get `hidden_size`-dimensional bert representation\n",
    "encoded = self.encoder(encoder_inputs).last_hidden_state\n",
    "logging.debug(f\"encoded.shape: {encoded.shape}\")\n",
    "\n",
    "# Downsample to d-representation\n",
    "downsampled = self.output_projection(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e44862cb-634f-4570-aaaf-620850c8f273",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'timestep'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m output\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'timestep'"
     ]
    }
   ],
   "source": [
    "output = model.forward(example_text, timestep=t)\n",
    "print(output.shape)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3615e-5013-4d6c-92a6-7095a3b6e552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
