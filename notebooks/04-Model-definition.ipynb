{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2266187-df83-47a0-afad-1331eb8d87d3",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "In this notebook I define the model object for the Diffusion LM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c0568787-98eb-4d56-9eb3-bab4b07f76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "from transformers.models.bert.modeling_bert import BertEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "e3ae250f-adf8-4915-a0b5-e5f7605af5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_noise_schedule(t, T=2000, s=1e-4):\n",
    "    alpha = 1 - np.sqrt(t / T + s)\n",
    "    return np.sqrt(1 - alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "fda92ac6-ff1d-41e1-99d0-bb75fe52e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim, max_period=10000):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(max_period)\n",
    "        * torch.arange(start=0, end=half, dtype=torch.float32)\n",
    "        / half\n",
    "    )\n",
    "    args = timesteps[:, None].float() * freqs[None]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "    if dim % 2:\n",
    "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "9f627082-68da-4061-b350-b32cb299ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionLM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model=\"bert-base-uncased\",\n",
    "        T=2000,  # diffusion steps\n",
    "        d=16,  # embedding dimensions\n",
    "        lr=1e-4,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(base_model)\n",
    "        self.embedding = nn.Embedding(self.tokenizer.vocab_size, d)\n",
    "        self.bert_config = BertConfig()\n",
    "        self.bert_model = BertModel(self.bert_config)\n",
    "        self.hidden_dim = d\n",
    "        self.time_embed_dim = 4 * d\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_size = self.bert_config.hidden_size\n",
    "        self.LayerNorm = nn.LayerNorm(\n",
    "            self.hidden_size, eps=self.bert_config.layer_norm_eps\n",
    "        )\n",
    "\n",
    "        # Add position embeddings\n",
    "        self.register_buffer(\n",
    "            \"position_ids\",\n",
    "            torch.arange(self.bert_config.max_position_embeddings).expand((1, -1)),\n",
    "        )\n",
    "        self.position_embeddings = self.bert_model.embeddings.position_embeddings\n",
    "\n",
    "        # Add time embedding\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            nn.Linear(d, self.time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.time_embed_dim, self.hidden_size),\n",
    "        )\n",
    "\n",
    "        # Downsample input vector\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Linear(d, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "        )\n",
    "\n",
    "        # Downsample output vector\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, d),\n",
    "        )\n",
    "\n",
    "    def forward(self, w, t):\n",
    "\n",
    "        # Convert text to tokens\n",
    "        tokens = model.tokenizer(w, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        seq_length = tokens.size(1)\n",
    "\n",
    "        # Get d-dimensional token embeddings\n",
    "        embeddings = self.embedding(encoded_input)\n",
    "\n",
    "        # Upsample to `hidden_size` dimensional embeddings\n",
    "        upsampled = self.input_projection(embeddings)\n",
    "        print(f\"upsampled.shape: {upsampled.shape}\")\n",
    "\n",
    "        # Add timestep embedding + unroll across each sequence\n",
    "        timesteps = self.time_embedding(timestep_embedding(t, self.hidden_dim))\n",
    "        timesteps = timesteps.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        print(f\"timestep.shape: {timesteps.shape}\")\n",
    "\n",
    "        # Calculate positional embedding\n",
    "        position_embeddings = self.position_embeddings(\n",
    "            self.position_ids[:, :seq_length]\n",
    "        )\n",
    "        print(f\"position_embeddings.shape: {position_embeddings.shape}\")\n",
    "\n",
    "        # Apply dropout + layernorm\n",
    "        encoder_inputs = self.dropout(\n",
    "            self.LayerNorm(upsampled + timesteps + position_embeddings)\n",
    "        )\n",
    "\n",
    "        # Get `hidden_size`-dimensional bert representation\n",
    "        representations = model.bert_model.encoder(encoder_inputs).last_hidden_state\n",
    "        print(f\"representations.shape: {representations.shape}\")\n",
    "\n",
    "        # Downsample to d-representation\n",
    "        downsampled = self.output_projection(representations)\n",
    "\n",
    "        return downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c2f505f9-a4e9-4900-84dd-0a1e659603cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffusionLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2901a741-e82a-4a7e-9ce6-5399b2fda4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsampled.shape: torch.Size([1, 14, 768])\n",
      "timestep.shape: torch.Size([1, 14, 768])\n",
      "position_embeddings.shape: torch.Size([1, 14, 768])\n",
      "representations.shape: torch.Size([1, 14, 768])\n",
      "torch.Size([1, 14, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1186,  0.1950,  0.0790,  0.0560, -0.3271, -0.1720,  0.4681,\n",
       "           0.0050,  0.1108, -0.0352, -0.3271, -0.1223,  0.2669,  0.5697,\n",
       "          -0.0176,  0.1616],\n",
       "         [-0.1581,  0.0814,  0.1467,  0.0228, -0.2938,  0.3322, -0.0022,\n",
       "           0.1411, -0.1313,  0.0730, -0.0896,  0.0159,  0.5176,  0.2286,\n",
       "           0.1918,  0.0072],\n",
       "         [ 0.0645,  0.4671,  0.1659, -0.1761, -0.1825, -0.0747,  0.1974,\n",
       "           0.1158, -0.0988, -0.0140,  0.1065,  0.0419,  0.6395,  0.3416,\n",
       "           0.3220,  0.1937],\n",
       "         [-0.1090,  0.1854,  0.2750, -0.0114, -0.2811, -0.0364,  0.3118,\n",
       "           0.0676,  0.0936,  0.0554, -0.3419,  0.0867,  0.4015,  0.3010,\n",
       "           0.1553,  0.0403],\n",
       "         [-0.2477, -0.0603,  0.1905, -0.2386, -0.3107,  0.0805,  0.1016,\n",
       "           0.0788,  0.0560,  0.0754, -0.0119, -0.1821,  0.3974,  0.0819,\n",
       "           0.1427, -0.1553],\n",
       "         [-0.0889,  0.2032,  0.0881, -0.4264, -0.1793, -0.2520, -0.1048,\n",
       "           0.0356,  0.0330, -0.2745,  0.0748, -0.1239,  0.4203,  0.4993,\n",
       "           0.4337, -0.1607],\n",
       "         [ 0.0587,  0.3992, -0.0934,  0.5395, -0.2207, -0.0281, -0.0190,\n",
       "           0.0314, -0.0372,  0.1452,  0.2904,  0.0082,  0.1534, -0.0267,\n",
       "          -0.1231,  0.0275],\n",
       "         [ 0.0758,  0.2937, -0.0845,  0.0551, -0.3974,  0.1413,  0.0290,\n",
       "           0.0182,  0.1059,  0.1746,  0.1254,  0.0093,  0.0813,  0.3284,\n",
       "           0.0269,  0.1907],\n",
       "         [-0.2878,  0.0990,  0.1202,  0.1998, -0.3079, -0.0767,  0.2922,\n",
       "           0.0207, -0.0941,  0.0756, -0.2017,  0.0773,  0.0454, -0.0267,\n",
       "           0.1652,  0.0643],\n",
       "         [-0.1460,  0.3383,  0.2205, -0.2136, -0.2173, -0.2928,  0.2954,\n",
       "           0.0198,  0.0104,  0.1458,  0.1604,  0.1142,  0.5816,  0.1585,\n",
       "           0.0871,  0.3564],\n",
       "         [-0.0561,  0.4795,  0.2731,  0.1141, -0.3317,  0.0530,  0.3748,\n",
       "           0.1361,  0.1490,  0.1653, -0.0583,  0.2079,  0.1209, -0.0045,\n",
       "           0.2824,  0.1535],\n",
       "         [-0.1036,  0.1804, -0.1658,  0.0911, -0.5179, -0.0462,  0.1781,\n",
       "           0.2117, -0.0574,  0.1390, -0.0949,  0.1174,  0.2727, -0.1148,\n",
       "          -0.1597, -0.1030],\n",
       "         [ 0.2161,  0.1414,  0.2136, -0.0880, -0.1802,  0.3523,  0.2375,\n",
       "           0.3165, -0.0938,  0.1079, -0.1448,  0.1435,  0.0425,  0.1095,\n",
       "           0.0182,  0.0148],\n",
       "         [-0.3563,  0.1655,  0.2178, -0.0341, -0.2278, -0.1415,  0.2448,\n",
       "          -0.0357,  0.1939, -0.1105, -0.0385, -0.0564,  0.0168,  0.2119,\n",
       "           0.1879,  0.1343]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# diffusion time step\n",
    "t = torch.tensor([0])\n",
    "example_text = \"In a hole in the ground there lived a hobbit\"\n",
    "output = model.forward(example_text, t=t)\n",
    "print(output.shape)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
